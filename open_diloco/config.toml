# Model configuration
path_model = "PrimeIntellect/llama-150m-fresh"

# Data configuration
dataset_name_or_path = "$C4"

# Training optimization
lr = 4e-4
per_device_train_batch_size = 16
total_batch_size = 32
precision = "fp16-mixed"
warmup_steps = 1000
max_steps = 100000

# Project and logging
project = "${EXP}-${WORLD_RANK}"

# Node GPU configuration
node_gpu_counts = [2, 2, 2]

# LoRA
lora = false

# Checkpoint configuration
[ckpt]
interval = 2000
path = "${EXP}_ckpt"

# Hivemind configuration
[hv]
local_steps = 50
averaging_timeout = 1800
skip_load_from_peers = true
initial_peers = ["$PEER"]
galaxy_size = 3
world_rank = "${WORLD_RANK}"
selective_layer_patterns = ["lm_head"]
