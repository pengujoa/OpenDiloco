# Model configuration
path_model = "PrimeIntellect/llama-150m-fresh"

# Data configuration
dataset_name_or_path = "$C4"

# Training optimization
lr = 4e-4
per_device_train_batch_size = 4
total_batch_size = 16
precision = "fp16-mixed"
warmup_steps = 1000
max_steps = 88000
adjust_local_steps = true
validation = true

# Project and logging
project = "${EXP}-${WORLD_RANK}"

# LoRA
lora = false

# Checkpoint configuration
[ckpt]
interval = 200000
path = "${EXP}_ckpt"

# Hivemind configuration
[hv]
local_steps = 50
averaging_timeout = 1800
skip_load_from_peers = true
initial_peers = ["$PEER"]
galaxy_size = 3
world_rank = "${WORLD_RANK}"
# selective_layer_patterns = ["model.layers.0", "model.layers.1", "lm_head"]
token_weighted_aggregation = true
max_outer_optimization_steps = 200
